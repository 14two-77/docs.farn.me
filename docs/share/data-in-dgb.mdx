---
sidebar_position: 1
---

# Data Sci role in DGB project

## Overview

If your data science teammate is **completely new** to both software and data, and only has **3.5 weeks**, the right strategy isnâ€™t to make him do â€œreal ML.â€
Instead, you help him produce **visible, measurable, and realistic work** that looks professional on a resume â€” without being technically overwhelming.

Letâ€™s design a **practical plan that maximizes impact per effort.**

---

### ðŸ—“ï¸ Overview: 3.5-Week Data Science Roadmap

| Week            | Focus                             | Goal                                                           |
| --------------- | --------------------------------- | -------------------------------------------------------------- |
| **Week 1**      | Learn foundations + get setup     | Understand project, learn basic tools, generate sample data    |
| **Week 2**      | Build synthetic datasets & EDA    | Produce charts, basic stats, correlations                      |
| **Week 3**      | Create simple analytics dashboard | Integrate with backend DB or CSV, visualize data interactively |
| **Half Week 4** | Polish + present results          | Make it look professional for resume & GitHub                  |

---

### ðŸŽ¯ Week 1: Setup & Basic Learning

#### ðŸ”§ Learning goals

- Understand basic software dev flow: `git`, VS Code, Python, Docker (just enough)
- Learn enough **Python + pandas + matplotlib**
- Learn what **EDA (Exploratory Data Analysis)** means

#### ðŸ§° Practice tasks

1. Create `data/` folder in the monorepo

   - Put notebook files inside (`/data/notebooks/`)

2. Write a notebook: `generate_synthetic_transactions.ipynb`

   - Simulate 500â€“2000 transaction records with columns like:

     ```
     user_id, amount, currency, transaction_type, status, timestamp
     ```

   - Add random distributions (normal, exponential, uniform)

3. Save results as `.csv` or insert into PostgreSQL

#### ðŸ“ˆ Resume impact (already usable)

> â€œGenerated synthetic financial transaction data for analytics in a simulated digital banking system.â€

---

### ðŸ“Š Week 2: Data Exploration (EDA)

#### ðŸ§° Tasks

1. Load the synthetic CSV into a Jupyter notebook
2. Use pandas & matplotlib to create plots:

   - Deposits vs Withdraws per day
   - Transaction volume trend
   - User spending histogram
   - Average balance per user

3. Compute simple stats:

   - Mean, median, max, min of transaction amounts
   - Percentage of failed transactions

4. Save all charts as `.png` into `/data/reports/`

#### ðŸª„ Optional +1

Add **correlation matrix** or **heatmap** (with seaborn). It looks great visually.

#### ðŸ“„ Resume line

> â€œPerformed exploratory data analysis (EDA) on synthetic transaction data using pandas and matplotlib to uncover trends and anomalies.â€

---

### ðŸ–¥ï¸ Week 3: Analytics Dashboard (visual & resume-visible)

#### Two options depending on skill & time:

###### ðŸŸ¢ **Easier: Streamlit app**

- Build a simple Streamlit dashboard:

  - Upload CSV or connect to PostgreSQL (read-only)
  - Show summary metrics + charts (matplotlib or plotly)

- Run locally at `localhost:8501`

This looks professional and interactive fast.

Example pages:

- â€œTransaction Summaryâ€
- â€œDaily Volume Trendâ€
- â€œTop Users by Transaction Countâ€

**Tech stack:** Streamlit, pandas, matplotlib/plotly.

###### ðŸŸ¡ Harder (if they progress fast):

- Build Grafana dashboard using Postgres connection from your database.
- Create 2â€“3 panels visualizing transaction activity.

---

#### ðŸ“„ Resume line

> â€œDeveloped interactive financial analytics dashboard using Streamlit to visualize synthetic ledger and transaction activity.â€

---

### ðŸ§‘â€ðŸ’» Week 4 (Final 3â€“4 days): Polish & Document

#### Deliverables:

1. `/data/README.md`

   - Describe: data generation, tools used, key insights

2. Screenshots of charts or Streamlit dashboard
3. One polished Jupyter Notebook with all work in clean markdown blocks
4. If time: Add short presentation slide with 2â€“3 charts for GitHub README

#### Resume summary:

> **Data Analyst (Simulated Project)** â€” Digital Banking
- Created and analyzed synthetic transaction data (1k+ records) simulating deposit, withdrawal, and transfer operations.
- Built interactive financial analytics dashboard using Streamlit and pandas.
- Conducted data exploration and visualization to identify trends and anomalies.

---

### ðŸ§© Folder structure

Inside monorepo:

```
digital-banking/
â”œâ”€â”€ auth-service/
â”œâ”€â”€ account-service/
â”œâ”€â”€ ledger-service/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ generate_data.ipynb
â”‚   â”‚   â”œâ”€â”€ eda.ipynb
â”‚   â”œâ”€â”€ reports/
â”‚   â”‚   â”œâ”€â”€ daily_volume.png
â”‚   â”‚   â”œâ”€â”€ transaction_heatmap.png
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
```

---

### ðŸ§  TL;DR: What he learns + gets for resume

| Category      | Skill Learned                    | Resume Keyword                               |
| ------------- | -------------------------------- | -------------------------------------------- |
| Programming   | Python, Git, Docker              | `Python`, `pandas`, `matplotlib`             |
| Data          | EDA, basic SQL                   | `Data analysis`, `EDA`, `SQL`                |
| Visualization | Streamlit / matplotlib           | `Data visualization`, `Dashboard`            |
| Software      | Integration with backend project | `Microservice`, `Digital banking simulation` |

Even without ML, thatâ€™s **a complete data analystâ€“type project** under 4 weeks â€” and 100% resume-relevant.

---

## Detailed plan

Goal: **produce one small but meaningful contribution** that fits naturally into your Go-based project and looks great on their resume.

---

## ðŸŽ¯ Final Goal

> **Build and document a data analytics module** that:
>
> 1. Collects and cleans mock user or system data from your backend (PostgreSQL).
> 2. Generates useful insights (like user activity, record growth, etc.).
> 3. Visualizes results (chart or dashboard).
> 4. Produces a clear technical report or Jupyter notebook to show understanding.

---

## ðŸ§­ Week-by-Week Plan (2 hrs/day)

### **Week 1 â€” Foundations & Setup**

**Goal:** Understand project + learn minimal Python, pandas, and SQL.

**Day 1â€“2**

- Read about project goals (you explain your backend: what data exists, why it matters).
- Learn Python basics (variables, loops, lists, dicts).

  - Resource: [Kaggle Python Course](https://www.kaggle.com/learn/python) (~4 hours total).

**Day 3â€“4**

- Learn pandas and Jupyter Notebook.

  - Resource: [Kaggle Pandas Course](https://www.kaggle.com/learn/pandas)

- Practice with small CSV datasets.

**Day 5â€“7**

- Learn SQL basics:

  - SELECT, WHERE, GROUP BY, JOIN
  - Tool: [Mode SQL Tutorial](https://mode.com/sql-tutorial/)

- Connect Python â†’ PostgreSQL with `psycopg2` or `SQLAlchemy`.

â± _Expected output:_
A notebook that loads a simple table from your projectâ€™s database (or mock DB) and shows a few queries.

---

### **Week 2 â€” Real Project Data Integration**

**Goal:** Connect to your projectâ€™s PostgreSQL DB and explore its schema.

**Day 8â€“9**

- With your help, inspect your database schema (`users`, `activities`, etc.).
- Learn how to use environment variables safely (no passwords in code).

**Day 10â€“11**

- Write Python scripts or a notebook to extract data (e.g., user signup logs or activity records).
- Clean it using pandas (handle nulls, dates, etc.).

**Day 12â€“14**

- Generate simple metrics:

  - Total users
  - Monthly growth
  - Activity count per region or church (for your HRM or ClinicChip project)

- Log results to console and save cleaned data as CSV.

â± _Expected output:_
A notebook that queries and summarizes your real or mock data.

---

### **Week 3 â€” Insights & Visualization**

**Goal:** Turn data into insight and present it clearly.

**Day 15â€“18**

- Learn `matplotlib` or `plotly` for visualization.
- Plot charts like:

  - User growth over time
  - Active vs inactive users
  - Most common activity types

**Day 19â€“21**

- Add meaningful text: interpret graphs, explain trends.
- Add one or two â€œbusiness logic insights,â€ e.g.:

  - â€œActivity participation drops after week 8 â€” may indicate engagement issues.â€
  - â€œMost users registered from X region.â€

â± _Expected output:_
A clean, labeled chart with short analysis text.

---

### **Half Week (Days 22â€“25) â€” Polish & Resume Impact**

**Goal:** Package results professionally.

**Day 22â€“23**

- Clean notebook (titles, sections, comments).
- Export to PDF or HTML report.

**Day 24â€“25**

- Write short README or summary:

  - Project goal
  - Data used
  - Techniques applied
  - Key findings
  - What could be improved with real data

---

## ðŸ§¾ Deliverables

At the end of 3.5 weeks:

1. `analysis.ipynb` â€” Clean, documented Jupyter notebook.
2. `README.md` â€” Short summary of what was done.
3. Optional `charts/` folder with exported images.
4. (Optional) CSV/JSON cleaned dataset.

---

## ðŸ’¼ Resume Line Example

> **Data Analytics Intern (Project Collaboration)** â€” _Built data analysis pipeline for internal HRM-like SaaS._
>
> - Connected PostgreSQL database to Python (pandas, SQLAlchemy).
> - Analyzed user activity and growth patterns; visualized insights with Plotly.
> - Delivered clean, reproducible notebook and dashboard for system monitoring.

---

